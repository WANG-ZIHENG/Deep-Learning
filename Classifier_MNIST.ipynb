{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61859129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if gpu/cpu\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374df5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths of download and result\n",
    "\n",
    "ROOT = './'\n",
    "\n",
    "DataPath = ROOT + 'Classifier-data/'\n",
    "ResultPath = ROOT + 'Classifier-results/'\n",
    "\n",
    "# make these directories if not available \n",
    "# --> this can help avoid to download again and again\n",
    "os.makedirs(DataPath, exist_ok=True)\n",
    "os.makedirs(ResultPath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST datasets\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root=DataPath,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root=DataPath,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19400d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data \n",
    "data, label = train_set[4]\n",
    "print(data.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn parameters of torch.utils.data.DataLoader(\n",
    "   # dataset train_set,\n",
    "   # amount of lines of each inputing time    batch_size=16,\n",
    "   # when each iteration, whether data need to be shuffle/disrupt   shuffle=True,\n",
    "   # amount of works, the best value is the amount of cpu/gpu of laptop num_workers=2\n",
    "#)\n",
    "\n",
    "# met mistakes about num_workers, in Jupyter num_worker = 0 is more friendly\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=24, # Forward pass only so batch size can be larger\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "classes = np.arange(0, 10)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some images and labels as a sanity check.\n",
    "# Use `torchvision.utils.make_grid` to create one image from a set of images. \n",
    "# Note that this function converts single channel (grey-scale) tensors to have three channels. \n",
    "# This is done by replicating the values into red, green and blue channels.\n",
    "\n",
    "\n",
    "def timshow(x):\n",
    "    # np.transpose can change axises order by order of the instruction(parameters)\n",
    "    xa = np.transpose(x.numpy(),(1,2,0))  \n",
    "    plt.imshow(xa)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return xa\n",
    "    \n",
    "# get a batch of random training examples (images and corresponding labels)\n",
    "dataiter = iter(train_loader)\n",
    "# images, labels = dataiter.next()\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images and labels\n",
    "print(images.size())\n",
    "timshow(torchvision.utils.make_grid(images))\n",
    "print(*labels.numpy())     # asterisk unpacks the ndarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6be64d",
   "metadata": {},
   "source": [
    "## Build a classifier\n",
    "Instead of defining the classifier function, loss function and parameter updates directly as we did in PyTorch.ipynb, it is convenient to use the `torch.nn` and `torch.optim` packages. These provide a simple way to build networks without losing sight of the iterative steps in gradient descent.\n",
    "\n",
    "First we construct the classifer function using the nn.Sequential wrapper that simply sequences the steps in the classifier function. In the case of a linear classifier there is just one nn.Linear layer. This is preceeded by `nn.Flatten` that vectorises a $28\\times28$ input image into a 1D vector of length $28*28$. We will also experiment with a two layer classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(32, 1, 5, 5)\n",
    "# With default parameters\n",
    "m = nn.Flatten()\n",
    "output = m(input)\n",
    "output.size()\n",
    "#torch.Size([32, 25])\n",
    "# With non-default parameters\n",
    "m = nn.Flatten(0, 2)\n",
    "output = m(input)\n",
    "output.size()\n",
    "#torch.Size([160, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one layer net\n",
    "\n",
    "net1 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # single layer\n",
    "    nn.Linear(28*28, 10)\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "#  .parameters()得到所有参数，第一次循环输出的是权重weight，与输入保持一致\n",
    "#  .parameters() gets all parameters, first loop figure out the weight(whose form is the same with inputs)\n",
    "# 第二次循环输出的是偏执bias数量，与输出数量保持一致\n",
    "# the second loop come out bias, whose form is the same with outputs\n",
    "for param in net1.parameters():\n",
    "    print(param.shape)\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two layers net\n",
    "# Sigmod between layers\n",
    "\n",
    "net2 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "\n",
    "    # single layer\n",
    "    #nn.Linear(28*28, 10)\n",
    "    \n",
    "    # two layers\n",
    "    nn.Linear(28*28, 300),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(300,10)\n",
    ")\n",
    "\n",
    "for param in net2.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df145ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 100    # number of epochs --> you can change as you want, for e.g., try 200 epochs\n",
    "net = net1       # 1-layer model\n",
    "results_path = ResultPath + 'linear1layer200epochs.pt'\n",
    "\n",
    "# initialise ndarray to store the mean loss in each epoch (on the training data)\n",
    "losses = np.zeros(nepochs)\n",
    "\n",
    "# Use a loss function and optimiser provided as part of PyTorch.\n",
    "# The chosen optimiser (Stochastic Gradient Descent with momentum) needs only to be given the parameters (weights and biases)\n",
    "# of the network and updates these when asked to perform an optimisation step below.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # initialise variables for mean loss calculation\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients to remove accumulated gradient from a previous iteration.\n",
    "        # 将所有参数梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # 运算后将梯度加入到grid属性中\n",
    "        # after calling, add all gradients into grids\n",
    "        loss.backward()\n",
    "        # 通过梯度下降优化参数，梯度由backward产生\n",
    "        # Gradients create by backward(), optimize parameters by gradient down\n",
    "        optimizer.step()\n",
    "    \n",
    "        # accumulate loss and increment minibatches\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "       \n",
    "    # record the mean loss for this epoch and show progress\n",
    "    losses[epoch] = running_loss / n\n",
    "    print(f\"epoch: {epoch+1} loss: {losses[epoch] : .3f}\")\n",
    "    \n",
    "# save network parameters and losses\n",
    "torch.save({\"state_dict\": net.state_dict(), \"losses\": losses}, results_path)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e94d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 100   # number of epochs --> you can change as you want\n",
    "net = net2       # 2-layer model\n",
    "results_path = ResultPath + '/linear2layer200epochs.pt'\n",
    "\n",
    "# initialise ndarray to store the mean loss in each epoch (on the training data)\n",
    "losses = np.zeros(nepochs)\n",
    "\n",
    "# Use a loss function and optimiser provided as part of PyTorch.\n",
    "# The chosen optimiser (Stochastic Gradient Descent with momentum) needs only to be given the parameters (weights and biases)\n",
    "# of the network and updates these when asked to perform an optimisation step below.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # initialise variables for mean loss calculation\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients to remove accumulated gradient from a previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # accumulate loss and increment minibatches\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "       \n",
    "    # record the mean loss for this epoch and show progress\n",
    "    losses[epoch] = running_loss / n\n",
    "    print(f\"epoch: {epoch+1} loss: {losses[epoch] : .3f}\")\n",
    "    \n",
    "# save network parameters and losses\n",
    "torch.save({\"state_dict\": net.state_dict(), \"losses\": losses}, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058df97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the first dimension of inputs and outputs corresponds to a minibatch of examples.\n",
    "print(f\"input size: {inputs.size()}, output size: {outputs.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the history of the loss function during training (mean loss in each epoch) for 1 and 2 layer models\n",
    "d1 = torch.load(ResultPath +  'linear1layer200epochs.pt')\n",
    "d2 = torch.load(ResultPath + 'linear2layer200epochs.pt')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(d1[\"losses\"], 'r', label = '1 layer', )\n",
    "plt.plot(d2[\"losses\"], 'g', label = '2 layers' )\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training loss for 1 and 2 layer classifiers')\n",
    "\n",
    "fig.savefig(ResultPath + \"training_loss_MNIST.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c39fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace() 主对角线的和  Sum of the main diagonal elements\n",
    "# np.diag() 对角线元素 The main diagonal elements\n",
    "\n",
    "def accuracy(cnfm):\n",
    "    return cnfm.trace()/cnfm.sum((0,1))\n",
    "\n",
    "def recalls(cnfm):\n",
    "    return np.diag(cnfm)/cnfm.sum(1)\n",
    "\n",
    "def precisions(cnfm):\n",
    "    return np.diag(cnfm)/cnfm.sum(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a40d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(ResultPath + 'linear2layer200epochs.pt')\n",
    "# now load the parameter state into the current model (make sure this is the right model).\n",
    "net.load_state_dict(d[\"state_dict\"])\n",
    "\n",
    "# initialise confusion matrix\n",
    "nclasses = classes.shape[0]\n",
    "cnfm = np.zeros((nclasses,nclasses),dtype=int)\n",
    "\n",
    "# work without gradient computation since we are testing (i.e. no optimisation)\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # find the class with the highest output.\n",
    "        # note that the outputs are confidence values since we didn't need to apply softmax in our network\n",
    "        # (nn.crossentropyloss takes raw condifence values and does its own softmax)   \n",
    "        _, predicted = torch.max(outputs, 1)    \n",
    "        #outputs返回的是10个类别的概率，选择概率最大的那个\n",
    "       ## 此处表示返回一个元组中有两个值，但是对第一个不感兴趣\n",
    "        #返回的元组的第一个元素是image data，即是最大的值；\n",
    "        #第二个元素是label，即是最大的值对应的索引。由于我们只需要label（最大值的索引），\n",
    "        #所以有 _ , predicted这样的赋值语句，表示忽略第一个返回值，把它赋值给 _，即舍弃它。\n",
    "         \n",
    "        #第二个参数 1 \n",
    "        #相当于取消掉列这个维度，在每行寻找最大值。\n",
    "        #axis = 0 表示纵轴， axis = 1 代表横轴\n",
    "            \n",
    "        # accumulate into confusion matrix\n",
    "        for i in range(labels.size(0)):\n",
    "            cnfm[labels[i].item(),predicted[i].item()] += 1\n",
    "              \n",
    "print(\"Confusion matrix\")\n",
    "print(cnfm)\n",
    "\n",
    "# show confusion matrix as a grey-level image\n",
    "plt.imshow(cnfm, cmap='gray')\n",
    "\n",
    "# show per-class recall and precision\n",
    "print(f\"Accuracy: {accuracy(cnfm) :.1%}\")\n",
    "r = recalls(cnfm)\n",
    "p = precisions(cnfm)\n",
    "for i in range(nclasses):\n",
    "    print(f\"Class {classes[i]} : Precision {p[i] :.1%}  Recall {r[i] :.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ee467",
   "metadata": {},
   "source": [
    "## Defining a bespoke model class\n",
    "\n",
    "In the above, we have used the 'container' module `nn.Sequential` to define our network. To give more flexibility in the definition of the network, we can replace this with our own `nn.module` as below. Notice here, we have used this flexibility to perform the flattening ourselves instead of using `nn.Flatten` - this will be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.reshape(x.size(0), -1) # flatten the input\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "    \n",
    "net = Classifier()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d816ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three layers net\n",
    "# ReLU between layers\n",
    "net3 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "\n",
    "    # single layer\n",
    "    #nn.Linear(28*28, 10)\n",
    "    \n",
    "    # three layers\n",
    "    nn.Linear(28*28, 300),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(300,50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,10)\n",
    "    \n",
    ")\n",
    "\n",
    "for param in net3.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 100\n",
    "net = net3\n",
    "results_path = ResultPath + '/linear3layer400epochs.pt'\n",
    "losses = np.zeros(nepochs)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "for epoch in range(nepochs):\n",
    "    running_loss = 0.0\n",
    "    n=0\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "    losses[epoch] = running_loss / n\n",
    "    print(f\"epoch: {epoch+1} loss: {losses[epoch] : .3f}\")\n",
    "torch.save({\"state_dict\": net.state_dict(), \"losses\": losses}, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(ResultPath + '/linear3layer400epochs.pt')\n",
    "# now load the parameter state into the current model (make sure this is the right model).\n",
    "net.load_state_dict(d[\"state_dict\"])\n",
    "\n",
    "# initialise confusion matrix\n",
    "nclasses = classes.shape[0]\n",
    "cnfm = np.zeros((nclasses,nclasses),dtype=int)\n",
    "\n",
    "# work without gradient computation since we are testing (i.e. no optimisation)\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # find the class with the highest output.\n",
    "        # note that the outputs are confidence values since we didn't need to apply softmax in our network\n",
    "        # (nn.crossentropyloss takes raw condifence values and does its own softmax)   \n",
    "        _, predicted = torch.max(outputs, 1)    \n",
    "        #outputs返回的是10个类别的概率，选择概率最大的那个\n",
    "       ## 此处表示返回一个元组中有两个值，但是对第一个不感兴趣\n",
    "        #返回的元组的第一个元素是image data，即是最大的值；\n",
    "        #第二个元素是label，即是最大的值对应的索引。由于我们只需要label（最大值的索引），\n",
    "        #所以有 _ , predicted这样的赋值语句，表示忽略第一个返回值，把它赋值给 _，即舍弃它。\n",
    "         \n",
    "        #第二个参数 1 \n",
    "        #相当于取消掉列这个维度，在每行寻找最大值。\n",
    "        #axis = 0 表示纵轴， axis = 1 代表横轴\n",
    "            \n",
    "        # accumulate into confusion matrix\n",
    "        for i in range(labels.size(0)):\n",
    "            cnfm[labels[i].item(),predicted[i].item()] += 1\n",
    "              \n",
    "print(\"Confusion matrix\")\n",
    "print(cnfm)\n",
    "\n",
    "# show confusion matrix as a grey-level image\n",
    "plt.imshow(cnfm, cmap='gray')\n",
    "\n",
    "# show per-class recall and precision\n",
    "print(f\"Accuracy: {accuracy(cnfm) :.1%}\")\n",
    "r = recalls(cnfm)\n",
    "p = precisions(cnfm)\n",
    "for i in range(nclasses):\n",
    "    print(f\"Class {classes[i]} : Precision {p[i] :.1%}  Recall {r[i] :.1%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
